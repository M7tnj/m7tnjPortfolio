<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Truth Detection: ML, Neuroscience & Linguistics</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/4.4.1/chart.umd.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    
    <!-- Chosen Palette: Academic Neutral -->
    <!-- Application Structure Plan: The application is a single-page, vertically-scrolling interactive report. This structure was chosen over a dashboard because the content is explanatory and best consumed in a logical, chapter-like flow. A sticky sidebar navigation (desktop) and a mobile menu provide a persistent table of contents, allowing the user (an academic) to either read linearly or jump to specific sections of interest. Key interactions include: 1) Clickable timeline items to reveal historical context. 2) Clickable cards to explore linguistic features. 3) Hover-to-reveal hotspots on a CSS-based brain diagram. 4) A live Chart.js chart demonstrating the P300 concept. 5) JS-powered filters for ML projects. 6) A tabbed interface to detail each proposed computational model, complete with CSS-based architecture diagrams. This structure is designed for exploration and deep understanding, directly addressing the user's research request. -->
    <!-- Visualization & Content Choices: 
        1.  Report Info (History): Goal: Show progression. Viz/Method: HTML/CSS list with JS click-to-reveal. Interaction: Click. Justification: Simple, text-heavy, avoids complex charts for sparse data. Library: Vanilla JS.
        2.  Report Info (Linguistic Cues): Goal: Categorize features. Viz/Method: HTML/CSS grid of cards. Interaction: Click-to-reveal details. Justification: Organizes distinct feature categories intuitively. Library: Vanilla JS.
        3.  Report Info (fMRI Brain Regions): Goal: Show key spatial areas. Viz/Method: CSS-based diagram (divs). Interaction: Hover-to-reveal info. Justification: Visualizes location (PFC, Insula) *without* using SVG. Library: Vanilla JS/CSS.
        4.  Report Info (EEG P300 Concept): Goal: Explain CIT vs. Innocent. Viz/Method: Chart.js Bar Chart (Canvas). Interaction: Tooltip. Justification: The bar chart is the clearest way to show the *signal difference* (the P300 spike) which is the core concept. Library: Chart.js.
        5.  Report Info (ML Projects): Goal: Organize examples. Viz/Method: Filterable HTML cards. Interaction: Button click (filter). Justification: Allows user to sort projects by modality (e.g., neuro, multi-modal). Library: Vanilla JS.
        6.  Report Info (Proposed Models): Goal: Detail computational approaches. Viz/Method: Tabbed interface with HTML/CSS flow diagrams. Interaction: Click tabs. Justification: Cleanly separates the 5 complex proposals; CSS diagrams show model flow without SVG/Mermaid. Library: Vanilla JS.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->

    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #fdfdfc;
            color: #333745;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 400px;
        }
        .sticky-nav a {
            transition: all 0.2s ease-in-out;
            border-left: 3px solid transparent;
        }
        .sticky-nav a.active, .sticky-nav a:hover {
            color: #0d9488;
            border-left-color: #0d9488;
            transform: translateX(4px);
        }
        .tab-btn {
            transition: all 0.2s ease-in-out;
        }
        .tab-btn.active {
            background-color: #0d9488;
            color: #ffffff;
        }
        .model-diagram {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 1rem;
            padding: 1rem;
            background-color: #f3f4f6;
            border-radius: 0.5rem;
            border: 1px solid #e5e7eb;
        }
        .model-diagram-row {
            display: flex;
            gap: 1rem;
            justify-content: center;
            width: 100%;
        }
        .model-block {
            background-color: #ffffff;
            border: 1px solid #d1d5db;
            padding: 0.75rem 1rem;
            border-radius: 0.375rem;
            text-align: center;
            font-weight: 600;
            color: #374151;
            box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
        }
        .model-arrow {
            font-size: 1.5rem;
            color: #6b7280;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .brain-diagram {
            position: relative;
            width: 250px;
            height: 250px;
            margin: 2rem auto;
            background-color: #e0e7ff;
            border: 2px solid #a5b4fc;
            border-radius: 45% 55% 60% 40% / 50% 45% 55% 50%;
            transform: rotate(-15deg);
        }
        .brain-hotspot {
            position: absolute;
            width: 30px;
            height: 30px;
            background-color: #ef4444;
            border: 2px solid #ffffff;
            border-radius: 50%;
            cursor: pointer;
            opacity: 0.7;
            transition: all 0.2s ease-in-out;
        }
        .brain-hotspot:hover {
            opacity: 1;
            transform: scale(1.2);
        }
        .hotspot-pfc { top: 20%; left: 60%; }
        .hotspot-insula { top: 45%; left: 45%; }
        .hotspot-info {
            display: none;
            position: absolute;
            bottom: 110%;
            left: 50%;
            transform: translateX(-50%);
            background-color: #333745;
            color: white;
            padding: 0.5rem 0.75rem;
            border-radius: 0.375rem;
            font-size: 0.875rem;
            width: 200px;
            z-index: 10;
        }
        .brain-hotspot:hover .hotspot-info {
            display: block;
        }
        [data-section] {
            padding-top: 6rem;
            margin-top: -4rem;
        }
    </style>
</head>
<body class="bg-stone-50 text-slate-800">

    <div class="max-w-7xl mx-auto">
        <div class="flex flex-col md:flex-row">

            <button id="mobile-menu-btn" class="md:hidden fixed top-4 right-4 z-50 bg-white p-2 rounded-md shadow-lg">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>

            <nav id="sidebar-nav" class="sticky-nav fixed md:sticky top-0 h-screen w-64 bg-stone-100 p-8 transform -translate-x-full md:translate-x-0 transition-transform z-40 md:z-10 md:top-0 md:h-screen md:w-1/4 lg:w-1/5 md:flex md:flex-col md:justify-center">
                <h3 class="text-xl font-bold text-teal-700 mb-6">Report Contents</h3>
                <div class="flex flex-col space-y-3">
                    <a href="#introduction" class="font-semibold text-slate-600 px-3 py-1">Introduction</a>
                    <a href="#history" class="font-semibold text-slate-600 px-3 py-1">Historical View</a>
                    <a href="#linguistic" class="font-semibold text-slate-600 px-3 py-1">Linguistic Cues</a>
                    <a href="#neuro" class="font-semibold text-slate-600 px-3 py-1">Neuro-Cognitive Signatures</a>
                    <a href="#ml_projects" class="font-semibold text-slate-600 px-3 py-1">Multi-modal ML Projects</a>
                    <a href="#proposals" class="font-semibold text-slate-600 px-3 py-1">Proposed Approaches</a>
                </div>
            </nav>

            <main class="w-full md:w-3/4 lg:w-4/5 p-6 md:p-12">

                <section id="introduction" data-section>
                    <h1 class="text-4xl md:text-5xl font-bold text-teal-800 mb-4">Truth Detection at the Nexus</h1>
                    <h2 class="text-2xl font-semibold text-slate-600 mb-6">Fusing ML, Neuroscience, and Linguistics</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-4">
                        The quest for reliable truth detection has long been a challenge. Traditional methods, like the polygraph, are fundamentally limited as they measure physiological arousal (e.g., heart rate, skin conductance) which is a *correlate* of stress, not deception itself. An innocent person can be stressed, and a practiced liar can remain calm. This report explores the modern frontier of veracity assessment, moving beyond simple arousal to target the *cognitive processes* of deception.
                    </p>
                    <p class="text-lg text-slate-700 leading-relaxed">
                        We are at the convergence of three powerful fields: <strong class="text-teal-700">Machine Learning (ML)</strong>, which can find complex patterns in data; <strong class="text-teal-700">Neuroscience (fMRI/EEG)</strong>, which can observe the brain's activity during the act of lying; and <strong class="text-teal-700">Linguistics</strong>, which analyzes the structure and content of speech. By fusing these domains, we can build models that are more accurate, robust, and insightful than any single-modality approach. This report outlines the history, key findings, and future computational approaches in this interdisciplinary field.
                    </p>
                </section>

                <section id="history" data-section>
                    <h2 class="text-3xl font-bold text-slate-800 mb-6">A Brief History of Deception Detection</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-8">
                        The journey from crude ordeals to modern machine learning has been a long one. The timeline below highlights the key technological and conceptual shifts. Click each era to learn more.
                    </p>
                    <div class="space-y-4" id="timeline-container">
                        
                        <div class="bg-white rounded-lg shadow-md border border-slate-200">
                            <button class="timeline-toggle w-full text-left p-5 font-bold text-xl text-teal-700 flex justify-between items-center">
                                <span>1900s-1930s: The Polygraph Era</span>
                                <span class="timeline-icon text-2xl">+</span>
                            </button>
                            <div class="timeline-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                Originating as a medical device to track pulse, the polygraph was adapted by figures like Larson and Keeler for law enforcement. It combines measures of blood pressure, breathing, and galvanic skin response (GSR). Its fundamental flaw remains: it measures stress, not lies.
                            </div>
                        </div>

                        <div class="bg-white rounded-lg shadow-md border border-slate-200">
                            <button class="timeline-toggle w-full text-left p-5 font-bold text-xl text-teal-700 flex justify-between items-center">
                                <span>1970s-1990s: Linguistic Analysis</span>
                                <span class="timeline-icon text-2xl">+</span>
                            </button>
                            <div class="timeline-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                Researchers began to systematically study the *language* of deception. Early work suggested that liars use different speech patterns, such as fewer "I" statements, more negative emotion words, and less complex sentences. This work laid the foundation for computational linguistics in veracity.
                            </div>
                        </div>

                        <div class="bg-white rounded-lg shadow-md border border-slate-200">
                            <button class="timeline-toggle w-full text-left p-5 font-bold text-xl text-teal-700 flex justify-between items-center">
                                <span>1980s-Present: The P300 (EEG)</span>
                                <span class="timeline-icon text-2xl">+</span>
                            </button>
                            <div class="timeline-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                The discovery of the P300 Event-Related Potential (ERP) provided a new tool. This is a brainwave spike that occurs ~300ms after a person recognizes a *rare and meaningful* stimulus. This led to the Concealed Information Test (CIT), where a guilty suspect's brain "gives away" recognition of crime details.
                            </div>
                        </div>

                        <div class="bg-white rounded-lg shadow-md border border-slate-200">
                            <button class="timeline-toggle w-full text-left p-5 font-bold text-xl text-teal-700 flex justify-between items-center">
                                <span>1990s-Present: fMRI & Cognitive Load</span>
                                <span class="timeline-icon text-2xl">+</span>
                            </button>
                            <div class="timeline-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                Functional Magnetic Resonance Imaging (fMRI) allowed researchers to see *which* brain regions activate during deception. Studies consistently show increased activity in the prefrontal cortex and other areas related to *executive control* and *cognitive load*. This reinforces the idea that lying is "hard work" for the brain, as it must invent a falsehood while inhibiting the truth.
                            </div>
                        </div>

                        <div class="bg-white rounded-lg shadow-md border border-slate-200">
                            <button class="timeline-toggle w-full text-left p-5 font-bold text-xl text-teal-700 flex justify-between items-center">
                                <span>2000s-Present: Machine Learning</span>
                                <span class="timeline-icon text-2xl">+</span>
                            </button>
                            <div class="timeline-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                With more data, machine learning (ML) models (like SVMs, Random Forests, and now Deep Learning) began to outperform human experts. ML can analyze hundreds of linguistic, vocal, and physiological features simultaneously. The current frontier is *multi-modal ML*, which fuses these different data streams for the most robust predictions.
                            </div>
                        </div>

                    </div>
                </section>

                <section id="linguistic" data-section>
                    <h2 class="text-3xl font-bold text-slate-800 mb-6">Linguistic Cues to Deception</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-8">
                        Language is a rich source of data. Deception often leaks through *how* we say something, not just *what* we say. Research has identified several consistent patterns. Click the categories below for details.
                    </p>
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6" id="linguistic-grid">
                        
                        <div class="bg-white rounded-lg shadow-md border border-slate-200 overflow-hidden">
                            <button class="linguistic-toggle w-full text-left p-5 font-bold text-xl text-teal-700">
                                Quantity & Complexity
                            </button>
                            <div class="linguistic-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                Deceivers often provide less information. Their stories tend to be shorter, with less complex sentence structures and a less diverse vocabulary. This is a sign of cognitive load—it's hard to invent a rich, detailed story.
                            </div>
                        </div>

                        <div class="bg-white rounded-lg shadow-md border border-slate-200 overflow-hidden">
                            <button class="linguistic-toggle w-full text-left p-5 font-bold text-xl text-teal-700">
                                Non-Immediacy
                            </button>
                            <div class="linguistic-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                Liars tend to psychologically distance themselves from their story. This manifests as using fewer first-person pronouns (e.g., "I," "me," "my") and using more linguistic "hedges" or modifiers (e.g., "maybe," "perhaps," "sort of").
                            </div>
                        </div>

                        <div class="bg-white rounded-lg shadow-md border border-slate-200 overflow-hidden">
                            <button class="linguistic-toggle w-full text-left p-5 font-bold text-xl text-teal-700">
                                Affect & Emotion
                            </button>
                            <div class="linguistic-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                Deceptive narratives often contain more negative emotion words (e.g., "sad," "angry," "hate"). This can be a sign of guilt or discomfort with the deception. They may also show a mismatch between stated emotion and vocal tone.
                            </div>
                        </div>

                        <div class="bg-white rounded-lg shadow-md border border-slate-200 overflow-hidden">
                            <button class="linguistic-toggle w-full text-left p-5 font-bold text-xl text-teal-700">
                                Specificity
                            </button>
                            <div class="linguistic-content hidden p-5 pt-0 text-slate-700 leading-relaxed">
                                Truthful accounts are often rich in specific, verifiable details (e.g., times, places, sensory information). Deceptive accounts tend to be more vague and general, as these details are harder to invent and maintain consistently.
                            </div>
                        </div>

                    </div>
                </section>

                <section id="neuro" data-section>
                    <h2 class="text-3xl font-bold text-slate-800 mb-6">Neuro-Cognitive Signatures</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-8">
                        Instead of measuring stress, neuroscience targets the brain itself. Deception is an executive function that creates a measurable cognitive load. The two primary tools for this are fMRI and EEG.
                    </p>
                    <div class="flex flex-col md:flex-row gap-8">
                        
                        <div class="w-full md:w-1/2 bg-white p-6 rounded-lg shadow-md border border-slate-200">
                            <h3 class="text-2xl font-bold text-teal-700 mb-4">fMRI: The "Where" of Deception</h3>
                            <p class="text-slate-700 mb-4">fMRI (Functional Magnetic Resonance Imaging) measures blood flow to different brain regions. Deception consistently activates areas associated with executive control and conflict monitoring.</p>
                            <p class="text-slate-700 mb-4"><strong>Key Regions:</strong> Hover over the hotspots on the diagram.</p>
                            <div class="brain-diagram">
                                <div class="brain-hotspot hotspot-pfc">
                                    <div class="hotspot-info"><strong>Prefrontal Cortex (PFC):</strong> Involved in working memory, planning, and *inhibiting* the truthful response.</div>
                                </div>
                                <div class="brain-hotspot hotspot-insula">
                                    <div class="hotspot-info"><strong>Anterior Insula:</strong> Linked to emotional awareness, conflict, and the subjective feeling of "wrongness" or arousal.</div>
                                </div>
                            </div>
                            <p class="text-slate-700 mt-4"><strong>Challenges:</strong> fMRI is extremely expensive, slow, and requires the subject to be perfectly still. Furthermore, it's hard to prove that the activation is *specific* to lying and not just to general cognitive effort or anxiety.</p>
                        </div>
                        
                        <div class="w-full md:w-1/2 bg-white p-6 rounded-lg shadow-md border border-slate-200">
                            <h3 class="text-2xl font-bold text-teal-700 mb-4">EEG: The "When" of Deception</h3>
                            <p class="text-slate-700 mb-4">EEG (Electroencephalography) measures electrical activity with millisecond-precision. Its most powerful use is in the <strong>Concealed Information Test (CIT)</strong>, which uses the <strong>P300</strong> brainwave.</p>
                            <p class="text-slate-700 mb-4">The P300 is a "recognition spike" that occurs when a person sees a rare, meaningful item. In a CIT, a suspect is shown items (e.g., potential murder weapons). An innocent person's brain treats all items as irrelevant. A guilty person's brain will involuntarily "spike" when they see the *actual* item they recognize from the crime.</p>
                            <div class="chart-container mt-6">
                                <canvas id="p300Chart"></canvas>
                            </div>
                        </div>
                    </div>
                </section>
                
                <section id="ml_projects" data-section>
                    <h2 class="text-3xl font-bold text-slate-800 mb-6">Multi-modal ML Projects</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-8">
                        The most promising results come from *fusing* data sources. Machine learning models, particularly deep learning, are adept at finding patterns across different modalities (e.g., text, audio, video, neuro-signals). Filter the examples below.
                    </p>
                    <div class="flex flex-wrap gap-2 mb-8" id="filter-container">
                        <button class="filter-btn bg-teal-600 text-white px-4 py-2 rounded-full font-semibold" data-filter="all">All</button>
                        <button class="filter-btn bg-slate-200 text-slate-700 px-4 py-2 rounded-full font-semibold" data-filter="neuro">Linguistic + Neuro</button>
                        <button class="filter-btn bg-slate-200 text-slate-700 px-4 py-2 rounded-full font-semibold" data-filter="physio">Linguistic + Physio/Behavioral</button>
                    </div>

                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6" id="projects-grid">
                        
                        <div class="project-card bg-white p-6 rounded-lg shadow-md border border-slate-200" data-type="physio">
                            <h3 class="text-xl font-bold text-teal-700 mb-2">Bimodal CNN (Text + Physiological)</h3>
                            <p class="text-slate-700">This approach uses a Convolutional Neural Network (CNN) to analyze two streams: 1) Linguistic features extracted from interview transcripts, and 2) Physiological data (like heart rate, GSR) averaged over time. Research shows this bimodal fusion significantly outperforms models trained on only one modality.</p>
                            <div class="mt-4 text-sm font-semibold text-slate-500">Modalities: Text, Physiological</div>
                        </div>

                        <div class="project-card bg-white p-6 rounded-lg shadow-md border border-slate-200" data-type="physio">
                            <h3 class="text-xl font-bold text-teal-700 mb-2">Real-time AVT Fusion (Audio + Video + Text)</h3>
                            <p class="text-slate-700">A system designed for real-time analysis, this model fuses three data streams. It extracts acoustic features (vocal pitch, jitter), visual features (facial micro-expressions, gaze), and linguistic features (from real-time transcription). Fusing these behavioral cues has achieved very high accuracy in high-stakes scenarios.</p>
                            <div class="mt-4 text-sm font-semibold text-slate-500">Modalities: Audio, Video, Text</div>
                        </div>
                        
                        <div class="project-card bg-white p-6 rounded-lg shadow-md border border-slate-200" data-type="neuro">
                            <h3 class="text-xl font-bold text-teal-700 mb-2">EEG-Linguistic Correlation</h3>
                            <p class="text-slate-700">These models aim to correlate EEG signals (like ERPs or frequency band power) with specific linguistic events. For example, does the brain show a conflict-related signal (like an N400 or P300) *immediately* after a subject uses a hedge word ("maybe") or a non-committal phrase? This tight temporal link is a powerful area of research.</p>
                            <div class="mt-4 text-sm font-semibold text-slate-500">Modalities: Text, EEG</div>
                        </div>

                        <div class="project-card bg-white p-6 rounded-lg shadow-md border border-slate-200" data-type="physio">
                            <h3 class="text-xl font-bold text-teal-700 mb-2">Gaze + Speech Attention Models</h3>
                            <p class="text-slate-700">Using an LSTM (Long Short-Term Memory) network with an attention mechanism, this model analyzes gaze patterns and speech features. The model learns that, for example, averted gaze *while* discussing a key topic, or a mismatch between gaze and semantic focus, is highly predictive of deception.</p>
                            <div class="mt-4 text-sm font-semibold text-slate-500">Modalities: Gaze, Audio, Text</div>
                        </div>
                    </div>
                </section>

                <section id="proposals" data-section>
                    <h2 class="text-3xl font-bold text-slate-800 mb-6">Proposed Computational Approaches</h2>
                    <p class="text-lg text-slate-700 leading-relaxed mb-8">
                        Based on this research, several advanced computational projects can be proposed. These models aim to create more robust, accurate, and insightful systems by leveraging multi-modal data and modern deep learning architectures.
                    </p>
                    
                    <div class="bg-white rounded-lg shadow-md border border-slate-200">
                        <div class="flex flex-wrap border-b border-slate-200" id="tabs-container">
                            <button class="tab-btn active font-semibold py-3 px-5" data-tab="model1">1. Fusion (EEG+Text)</button>
                            <button class="tab-btn font-semibold py-3 px-5 text-slate-600" data-tab="model2">2. Cognitive Load (Vocal+Gaze)</button>
                            <button class="tab-btn font-semibold py-3 px-5 text-slate-600" data-tab="model3">3. 3D-CNN (fMRI)</button>
                            <button class="tab-btn font-semibold py-3 px-5 text-slate-600" data-tab="model4">4. GAN Deception Generator</button>
                            <button class="tab-btn font-semibold py-3 px-5 text-slate-600" data-tab="model5">5. P300 Feature Classifier</button>
                        </div>

                        <div class="p-6">
                            <div class="tab-content" id="model1">
                                <h3 class="text-2xl font-bold text-teal-700 mb-4">Model 1: Multi-modal Fusion (EEG+Text)</h3>
                                <p class="text-slate-700 mb-4"><strong>Concept:</strong> A deep learning model that simultaneously processes linguistic content and time-locked EEG data. The model would learn not just *what* is said, but also the brain's real-time reaction to saying it.</p>
                                <p class="text-slate-700 mb-4"><strong>Data Inputs:</strong> 1) Synchronized, pre-processed EEG time-series data. 2) Transcribed text, aligned with the EEG data.</p>
                                <p class="text-slate-700 mb-4"><strong>Architecture:</strong> A two-branch network:</p>
                                <div class="model-diagram my-6">
                                    <div class="model-diagram-row">
                                        <div class="model-block">EEG Time-Series</div>
                                        <div class="model-block">Transcribed Text</div>
                                    </div>
                                    <div class="model-diagram-row">
                                        <div class="model-arrow">↓</div>
                                        <div class="model-arrow">↓</div>
                                    </div>
                                    <div class="model-diagram-row">
                                        <div class="model-block">1D CNN / LSTM</div>
                                        <div class="model-block">BERT / Transformer</div>
                                    </div>
                                    <div class="model-diagram-row">
                                        <div class="model-arrow">↘</div>
                                        <div class="model-arrow">↙</div>
                                    </div>
                                    <div class="model-block">Concatenated Feature Vector</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">MLP Classifier</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block font-bold text-teal-700">Prediction (Truth / Deception)</div>
                                </div>
                                <p class="text-slate-700"><strong>Key Challenges:</strong> Precise time-synchronization of text and EEG signals. High signal-to-noise ratio required for EEG. Large, labeled datasets are scarce.</p>
                            </div>

                            <div class="tab-content hidden" id="model2">
                                <h3 class="text-2xl font-bold text-teal-700 mb-4">Model 2: Real-time Cognitive Load Monitor (Vocal+Gaze)</h3>
                                <p class="text-slate-700 mb-4"><strong>Concept:</strong> A model that *infers* cognitive load, a strong proxy for deception, by analyzing non-invasive behavioral cues: vocal prosody and eye-tracking.</p>
                                <p class="text-slate-700 mb-4"><strong>Data Inputs:</strong> 1) Raw audio stream (for vocal features like pitch, jitter, shimmer, speech pace). 2) Eye-tracking data (saccade frequency, pupil dilation, gaze aversion).</p>
                                <p class="text-slate-700 mb-4"><strong>Architecture:</strong> A time-series model (like an RNN or LSTM) that processes features extracted from both streams.</p>
                                <div class="model-diagram my-6">
                                    <div class="model-diagram-row">
                                        <div class="model-block">Audio Stream</div>
                                        <div class="model-block">Eye-Tracker Data</div>
                                    </div>
                                    <div class="model-diagram-row">
                                        <div class="model-arrow">↓</div>
                                        <div class="model-arrow">↓</div>
                                    </div>
                                    <div class="model-diagram-row">
                                        <div class="model-block">Feature Extraction (e.g., MFCCs, Pitch)</div>
                                        <div class="model-block">Feature Extraction (e.g., Saccades)</div>
                                    </div>
                                    <div class="model-diagram-row">
                                        <div class="model-arrow">↘</div>
                                        <div class="model-arrow">↙</div>
                                    </div>
                                    <div class="model-block">Fused Time-Series Features</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">LSTM / GRU Network</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block font-bold text-teal-700">Real-time Cognitive Load Score</div>
                                </div>
                                <p class="text-slate-700"><strong>Key Challenges:</strong> Real-time feature extraction is computationally intensive. Gaze data can be noisy. Calibrating a "baseline" cognitive load for each individual.</p>
                            </div>

                            <div class="tab-content hidden" id="model3">
                                <h3 class="text-2xl font-bold text-teal-700 mb-4">Model 3: 3D Convolutional Neural Network (fMRI)</h3>
                                <p class="text-slate-700 mb-4"><strong>Concept:</strong> Leverage the full spatial *and* temporal information in an fMRI scan. Instead of analyzing pre-selected regions, a 3D-CNN can learn complex 4D spatio-temporal patterns across the entire brain that predict deception.</p>
                                <p class="text-slate-700 mb-4"><strong>Data Inputs:</strong> Full 4D fMRI BOLD signal data (3D brain volume over time), labeled by trial (truth/deception).</p>
                                <p class="text-slate-700 mb-4"><strong>Architecture:</strong> A 3D-CNN (or a Video-CNN architecture) that treats the time-series of brain scans as a "video."</p>
                                <div class="model-diagram my-6">
                                    <div class="model-block">4D fMRI Scan Data (x, y, z, time)</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">(3D + time) Convolutional Layers</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">Pooling & Flattening</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">Fully Connected Classifier</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block font-bold text-teal-700">Prediction (Truth / Deception)</div>
                                </div>
                                <p class="text-slate-700"><strong>Key Challenges:</strong> Extremely high dimensionality of data. Very small datasets (fMRI studies are expensive). High risk of overfitting. Disentangling "deception" patterns from "task-related" patterns.</p>
                            </div>
                            
                            <div class="tab-content hidden" id="model4">
                                <h3 class="text-2xl font-bold text-teal-700 mb-4">Model 4: Adversarial Deception Generator (GAN)</h3>
                                <p class="text-slate-700 mb-4"><strong>Concept:</strong> Use a Generative Adversarial Network (GAN) to build a more robust *detector*. A "Generator" network tries to create deceptive text that *mimics* truthful language. A "Discriminator" network (our detector) is trained to distinguish real truthful text from the Generator's "fake" deceptive text.</p>
                                <p class="text-slate-700 mb-4"><strong>Data Inputs:</strong> Large corpus of known truthful text and known deceptive text.</p>
                                <p class="text-slate-700 mb-4"><strong>Architecture:</strong> A classic GAN setup, likely using Transformer-based models (like GPT/BERT) as the Generator and Discriminator.</p>
                                <div class="model-diagram my-6">
                                    <div class="model-block">Noise + Truthful Context</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">Generator (e.g., GPT-2)</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">"Fake" Deceptive Text</div>
                                    <div class="model-arrow">↘</div>
                                    <div class="model-block">Discriminator (e.g., BERT)</div>
                                    <div class="model-arrow">↙</div>
                                    <div class="model-block">"Real" Deceptive Text</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block font-bold text-teal-700">Prediction (Real / Fake)</div>
                                </div>
                                <p class="text-slate-700"><strong>Key Challenges:</strong> Training GANs for text is notoriously difficult. Defining a clear "style" of deception for the generator to learn. The model might learn to exploit dataset biases rather than true deceptive cues.</p>
                            </div>

                            <div class="tab-content hidden" id="model5">
                                <h3 class="text-2xl font-bold text-teal-700 mb-4">Model 5: P300 Feature-based Classifier (EEG)</h3>
                                <p class="text-slate-700 mb-4"><strong>Concept:</strong> A more "classic" ML approach that is highly effective and interpretable. Instead of feeding raw EEG to a deep learning model, this extracts well-understood features from the P300 component (e.g., amplitude, latency) and feeds them to a powerful but simpler classifier.</p>
                                <p class="text-slate-700 mb-4"><strong>Data Inputs:</strong> Epoched EEG data, time-locked to the "probe" (crime detail) and "irrelevant" stimuli from a CIT.</p>
                                <p class="text-slate-700 mb-4"><strong>Architecture:</strong> A feature-extraction pipeline followed by a classifier.</p>
                                <div class="model-diagram my-6">
                                    <div class="model-block">EEG Epochs (Probe vs. Irrelevant)</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">Feature Extraction Pipeline</div>
                                    <div class="model-block" style="background-color: #f3f4f6; border: none; box-shadow: none; font-weight: 400; text-align: left; padding: 0 1rem;">
                                        - P300 Amplitude
                                        <br>- P300 Latency
                                        <br>- Frequency Band Power (Alpha, Theta)
                                    </div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">Feature Vector</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block">XGBoost / SVM / Random Forest</div>
                                    <div class="model-arrow">↓</div>
                                    <div class="model-block font-bold text-teal-700">Prediction (Guilty / Innocent)</div>
                                </div>
                                <p class="text-slate-700"><strong>Key Challenges:</strong> Susceptible to countermeasures (though new protocols reduce this). Signal quality is paramount. Requires a strict, controlled CIT paradigm, which isn't always possible.</p>
                            </div>
                        </div>
                    </div>
                </section>
            </main>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            
            const p300Ctx = document.getElementById('p300Chart').getContext('2d');
            new Chart(p300Ctx, {
                type: 'bar',
                data: {
                    labels: ['Irrelevant Item 1', 'Irrelevant Item 2', 'Crime Detail (Probe)', 'Irrelevant Item 3', 'Irrelevant Item 4'],
                    datasets: [
                        {
                            label: 'Innocent Subject',
                            data: [10, 12, 11, 9, 13],
                            backgroundColor: 'rgba(59, 130, 246, 0.6)',
                            borderColor: 'rgba(59, 130, 246, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'Guilty Subject (P300)',
                            data: [11, 9, 35, 10, 12],
                            backgroundColor: 'rgba(239, 68, 68, 0.6)',
                            borderColor: 'rgba(239, 68, 68, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Brainwave Amplitude (µV)'
                            }
                        }
                    },
                    plugins: {
                        legend: {
                            position: 'bottom',
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    let label = context.dataset.label || '';
                                    if (label) {
                                        label += ': ';
                                    }
                                    if (context.parsed.y !== null) {
                                        label += context.parsed.y + ' µV';
                                    }
                                    return label;
                                }
                            }
                        }
                    }
                }
            });

            const timelineToggles = document.querySelectorAll('.timeline-toggle');
            timelineToggles.forEach(toggle => {
                toggle.addEventListener('click', () => {
                    const content = toggle.nextElementSibling;
                    const icon = toggle.querySelector('.timeline-icon');
                    const isHidden = content.classList.contains('hidden');
                    
                    document.querySelectorAll('.timeline-content').forEach(c => c.classList.add('hidden'));
                    document.querySelectorAll('.timeline-icon').forEach(i => i.textContent = '+');
                    
                    if (isHidden) {
                        content.classList.remove('hidden');
                        icon.textContent = '−';
                    }
                });
            });
            
            const linguisticToggles = document.querySelectorAll('.linguistic-toggle');
            linguisticToggles.forEach(toggle => {
                toggle.addEventListener('click', () => {
                    const content = toggle.nextElementSibling;
                    content.classList.toggle('hidden');
                });
            });

            const filterBtns = document.querySelectorAll('.filter-btn');
            const projectCards = document.querySelectorAll('.project-card');
            filterBtns.forEach(btn => {
                btn.addEventListener('click', () => {
                    const filter = btn.getAttribute('data-filter');
                    
                    filterBtns.forEach(b => {
                        b.classList.remove('bg-teal-600', 'text-white');
                        b.classList.add('bg-slate-200', 'text-slate-700');
                    });
                    btn.classList.add('bg-teal-600', 'text-white');
                    btn.classList.remove('bg-slate-200', 'text-slate-700');
                    
                    projectCards.forEach(card => {
                        if (filter === 'all' || card.getAttribute('data-type') === filter) {
                            card.style.display = 'block';
                        } else {
                            card.style.display = 'none';
                        }
                    });
                });
            });

            const tabBtns = document.querySelectorAll('.tab-btn');
            const tabContents = document.querySelectorAll('.tab-content');
            tabBtns.forEach(btn => {
                btn.addEventListener('click', () => {
                    const tab = btn.getAttribute('data-tab');
                    
                    tabBtns.forEach(b => {
                        b.classList.remove('active');
                        b.classList.add('text-slate-600');
                    });
                    btn.classList.add('active');
                    btn.classList.remove('text-slate-600');
                    
                    tabContents.forEach(content => {
                        if (content.id === tab) {
                            content.classList.remove('hidden');
                        } else {
                            content.classList.add('hidden');
                        }
                    });
                });
            });

            const navLinks = document.querySelectorAll('.sticky-nav a');
            const sections = document.querySelectorAll('[data-section]');
            const mobileMenuBtn = document.getElementById('mobile-menu-btn');
            const sidebarNav = document.getElementById('sidebar-nav');

            mobileMenuBtn.addEventListener('click', () => {
                sidebarNav.classList.toggle('-translate-x-full');
            });

            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href') === `#${entry.target.id}`) {
                                link.classList.add('active');
                            }
                        });
                    }
                });
            }, { rootMargin: '-30% 0px -70% 0px', threshold: 0 });

            sections.forEach(section => {
                observer.observe(section);
            });

            navLinks.forEach(link => {
                link.addEventListener('click', (e) => {
                    if (window.innerWidth < 768) {
                        sidebarNav.classList.add('-translate-x-full');
                    }
                });
            });

        });
    </script>
</body>
</html>
